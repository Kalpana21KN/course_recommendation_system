# -*- coding: utf-8 -*-
"""online_course_recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xHNxznl2VaH_WhbPhdFSHUV68KGR4yae
"""

#load the dataset
import pandas as pd
df = pd.read_excel('online_course_recommendation_v2.xlsx')


df.shape

df.columns

df.head()

df.info()

df.isnull().sum()

df.describe()

df.duplicated().sum()

df.drop_duplicates()

df.nunique()

df['course_name'].unique()

df.dropna()

# Select object columns
object_columns = df.select_dtypes(include=['object']).columns

# Check for empty or whitespace-only strings
empty_or_whitespace_counts = {
    col: df[col].apply(lambda x: isinstance(x, str) and x.strip() == '').sum()
    for col in object_columns
}

# Display results
for col, count in empty_or_whitespace_counts.items():
    print(f"{col}: {count} empty or whitespace-only entries")

# Detect constant columns (same value for all rows)
constant_columns = [col for col in df.columns if df[col].nunique(dropna=False) == 1]

# Display result using if-else
if constant_columns:
    print("Constant columns (zero variance):")
    for col in constant_columns:
        print(f" {col}: {df[col].unique()[0]}")
else:
    print("No constant columns found. All columns have varying values.")

# Memory usage before optimization (in MB)
mem_before = df.memory_usage(deep=True).sum() / 1024**2

# Step 1: Downcast numeric columns
df_optimized = df.copy()
for col in df_optimized.select_dtypes(include=['int64']).columns:
    df_optimized[col] = pd.to_numeric(df_optimized[col], downcast='integer')

for col in df_optimized.select_dtypes(include=['float64']).columns:
    df_optimized[col] = pd.to_numeric(df_optimized[col], downcast='float')

# Step 2: Convert object columns to category (if low cardinality)
for col in df_optimized.select_dtypes(include=['object']).columns:
    if df_optimized[col].nunique() / len(df_optimized[col]) < 0.5:
        df_optimized[col] = df_optimized[col].astype('category')

# Memory usage after optimization (in MB)
mem_after = df_optimized.memory_usage(deep=True).sum() / 1024**2
# Memory reduction percentage
reduction_percent = round(((mem_before - mem_after) / mem_before) * 100, 2)

# Output
print(f"Memory Before: {mem_before:.2f} MB")
print(f"Memory After: {mem_after:.2f} MB")
print(f"Reduction: {reduction_percent}%")

# Get columns where number of unique values is 1
constant_columns = df.columns[df.nunique(dropna=False) == 1]

print("Constant columns:", list(constant_columns))

numerical_cols = df.select_dtypes(include=['int64', 'float64', 'int32', 'float32'])
numerical_cols.columns

# import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Plot histograms for each numerical column
for col in numerical_cols:
    plt.figure(figsize=(8, 4))
    sns.histplot(df[col], kde=True, bins=30, color='skyblue')
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Compute skewness for numerical columns only
skewness = df.skew(numeric_only=True)

# Display skewness in descending order
print("Skewness of numeric columns:")
print(skewness.sort_values(ascending=False))

# 1. Basic stats for numerical columns
print("Numerical Column Summary:\n")
print(df.describe(include='number').T)

# 2. Count unique values (for range check & imbalance in categories)
print("\n Unique Value Counts:\n")
for col in df.columns:
    print(f"{col}: {df[col].nunique()} unique values")

# 3. Detect extreme values (possible outliers)
print("\n Potential Outliers (Min/Max check):\n")
for col in df.select_dtypes(include='number').columns:
    min_val = df[col].min()
    max_val = df[col].max()
    print(f"{col}: Min = {min_val}, Max = {max_val}")

# 4. Detect category imbalance (if applicable)
print("\n Value Counts for Potential Class Columns:\n")
for col in df.select_dtypes(include='object').columns:
    print(f"\n{col}:\n{df[col].value_counts(normalize=True).head()}")

for col in numerical_cols:
    plt.figure(figsize=(6, 3))
    sns.kdeplot(df[col], fill=True, color='skyblue')
    plt.title(f"KDE Plot: {col}")
    plt.xlabel(col)
    plt.tight_layout()
    plt.show()

categorical_cols = df.select_dtypes(include=['object', 'category'])
categorical_cols.columns

# df['course_name'].value_counts()
# df['instructor'].value_counts()
# df['certification_offered'].value_counts()
df['difficulty_level'].value_counts()
#df['study_material_available'].value_counts()

for col in categorical_cols:
    plt.figure(figsize=(6, 4))
    sns.countplot(data=df, x=col, order=df[col].value_counts().index)
    plt.title(f'Distribution of {col}', fontsize=14)
    plt.xlabel(col, fontsize=12)
    plt.ylabel('Count', fontsize=12)
    plt.xticks(rotation=45)  # Rotate x labels if necessary
    plt.tight_layout()
    plt.show()

# Count unique values in each column
unique_counts = df.nunique().sort_values(ascending=False)

# Display the top N columns with the highest cardinality
print(" Unique Value Counts for Each Column:\n")
print(unique_counts)

# Optionally filter for high-cardinality features (e.g., > 50 unique values)
threshold = 50
high_cardinality = unique_counts[unique_counts > threshold]

print("\n High-Cardinality Features (more than 50 unique values):\n")
print(high_cardinality)

import matplotlib.pyplot as plt

# Identify categorical columns
categorical_cols = df.select_dtypes(include='object').columns

# Loop through each categorical column
for col in categorical_cols:
    if df[col].nunique() <= 5:  # Only for columns with 5 or fewer unique values
        plt.figure(figsize=(5, 5))
        df[col].value_counts().plot.pie(
            autopct='%1.1f%%',
            startangle=90,
            wedgeprops={'linewidth': 1, 'edgecolor': 'white'}
        )
        plt.title(f'{col} Distribution (Pie Chart)')
        plt.ylabel('')  # Remove default y-axis label
        plt.tight_layout()
        plt.show()

# Count unique values in each categorical column
cardinality_check = df[categorical_cols].nunique().sort_values()

print(" Categorical Column Cardinality:\n")
print(cardinality_check)

import seaborn as sns
import matplotlib.pyplot as plt

# Define important categorical and numeric columns
categorical_cols = ['difficulty_level','certification_offered','study_material_available']
numeric_cols = ['course_duration_hours','course_price','feedback_score','rating','enrollment_numbers']

# Generate box plots for each pair
for cat in categorical_cols:
    for num in numeric_cols:
        plt.figure(figsize=(8,5))
        sns.boxplot(x=df[cat], y=df[num])
        plt.title(f"Box Plot: {num} by {cat}")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Example with course_level and course_category
sns.countplot(data=df, x='difficulty_level', hue='course_name')
plt.title('Course Category Distribution by Course Level')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Select only numeric columns
numeric_df = df.select_dtypes(include=['int64', 'float64'])

# Compute correlation matrix
corr_matrix = numeric_df.corr()

# Display correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", square=True, linewidths=0.5)
plt.title("Feature Correlation Matrix (Multicollinearity Check)")
plt.tight_layout()
plt.show()

# Extract correlated pairs above threshold
threshold = 0.8
corr_pairs = corr_matrix.abs().unstack().sort_values(ascending=False)

# Remove self-pairs (1.0 correlation)
corr_pairs = corr_pairs[corr_pairs < 1]

# Filter only strong correlations
strong_corrs = corr_pairs[corr_pairs > threshold]

print("\nStrongly correlated feature pairs (|corr| > 0.8):\n")
print(strong_corrs.drop_duplicates())

import numpy as np

# Step 1: Compute correlation matrix
corr_matrix = df.select_dtypes(include=['int64', 'float64']).corr().abs()

# Step 2: Create a mask for upper triangle (to avoid duplicate pairs)
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# Step 3: Find features with correlation above threshold
threshold = 0.8
to_drop = [column for column in upper.columns if any(upper[column] > threshold)]

# Step 4: Drop them from the dataset
df_reduced = df.drop(columns=to_drop)

# Print what we dropped
print(" Dropped the following redundant features due to high correlation:")
print(to_drop)

# Store outlier information
outlier_summary = {}

# Loop through numeric columns
for col in df.select_dtypes(include=['int64', 'float64']):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Find number of outliers
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]

    outlier_summary[col] = {
        'Q1': Q1,
        'Q3': Q3,
        'IQR': IQR,
        'Lower Bound': lower_bound,
        'Upper Bound': upper_bound,
        'Outlier Count': outliers.shape[0],
        'Outlier %': round((outliers.shape[0] / df.shape[0]) * 100, 2)
    }

# Convert summary to DataFrame
outlier_df = pd.DataFrame(outlier_summary).T
outlier_df = outlier_df.sort_values(by='Outlier Count', ascending=False)

# Display results
outlier_df

import seaborn as sns
import matplotlib.pyplot as plt

# Set figure size
plt.figure(figsize=(15, 6))

# Loop through numeric columns and plot boxplots
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns

for i, col in enumerate(numeric_cols, 1):
    plt.subplot(1, len(numeric_cols), i)
    sns.boxplot(y=df[col], color='lightblue')
    plt.title(col)
    plt.tight_layout()

plt.suptitle('Boxplots for Outlier Detection', fontsize=16, y=1.05)
plt.show()

from scipy.stats import zscore
import numpy as np
import pandas as pd

# Calculate z-scores for numeric columns
z_scores = df.select_dtypes(include=['int64', 'float64']).apply(zscore)

# Identify rows where any column has a z-score > 3 or < -3
outliers = (np.abs(z_scores) > 3).any(axis=1)

# Count of outliers
print(f" Total rows with outliers (Z-score > 3): {outliers.sum()}")

# Optional: View these rows
df_outliers = df[outliers]
df_outliers.head()

# Remove outliers based on z-score
df_clean = df[~outliers]
df_clean.head()

# Collect illogical values
issues = {}

# Example: Rating > 5 or < 0
issues['rating_invalid'] = df[(df['rating'] > 5) | (df['rating'] < 0)]

# Example: Negative or 0 duration
issues['duration_invalid'] = df[df['course_duration_hours'] <= 0]

# Example: Negative time spent
issues['time_spent_invalid'] = df[df['time_spent_hours'] < 0]

# Example: Negative price
issues['price_invalid'] = df[df['course_price'] < 0]

# Example: Negative enrollment numbers
issues['enrollments_invalid'] = df[df['enrollment_numbers'] < 0]

# Summary
for k, v in issues.items():
    print(f"\n {k}: {len(v)} invalid rows")
    display(v.head())

# Threshold for "mostly null"
null_threshold = 0.95

# Find columns where > 95% values are null
mostly_null_cols = df.columns[df.isnull().mean() > null_threshold].tolist()

# Find constant columns (already done earlier)
constant_cols = [col for col in df.columns if df[col].nunique(dropna=False) == 1]

# Combine lists
cols_to_drop = list(set(mostly_null_cols + constant_cols))

print(" Columns suggested for dropping:")
for col in cols_to_drop:
    print(f"• {col}")

# Drop them (optional)
df = df.drop(columns=cols_to_drop)

for col in numeric_cols:
    if df[col].isnull().sum() > 0:
        if abs(df[col].skew()) < 1:
            df[col].fillna(df[col].mean(), inplace=True)
            print(f" {col} filled with mean")
        else:
            df[col].fillna(df[col].median(), inplace=True)
            print(f" {col} filled with median")
    else:
        print(f" {col} has no missing values")

# Get categorical columns
categorical_cols = df.select_dtypes(include='object').columns

# Check missing values
print(" Missing values in categorical columns:\n")
print(df[categorical_cols].isnull().sum())

# Impute with mode (most frequent value)
for col in categorical_cols:
    if df[col].isnull().sum() > 0:
        mode_value = df[col].mode()[0]
        df[col].fillna(mode_value, inplace=True)
        print(f" {col} filled with mode: '{mode_value}'")
    else:
        print(f" {col} has no missing values")

"""**Model** **Building**

Popularity based recommendation

The criteria based on:

1.courses with highest rating

2.number of enrollment

3.Feedback score
"""

# Normalize popularity indicators
df_clean['normalized_enrollment'] = (df_clean['enrollment_numbers'] - df_clean['enrollment_numbers'].min()) / (df_clean['enrollment_numbers'].max() - df_clean['enrollment_numbers'].min())
df_clean['normalized_feedback'] = (df_clean['feedback_score'] - df_clean['feedback_score'].min()) / (df_clean['feedback_score'].max() - df_clean['feedback_score'].min())

#  Create a Popularity Score
w1, w2, w3 = 0.5, 0.3, 0.2  # weights for enrollment, rating, feedback
df_clean['popularity_score'] = (w1 * df_clean['normalized_enrollment']) + (w2 * df_clean['rating'] / 5) + (w3 * df_clean['normalized_feedback'])

#  Sort by Popularity Score
popular_courses = df_clean.sort_values(by='popularity_score', ascending=False)

#  Get Top N Popular Courses
def get_top_popular_courses(n=10):
    return popular_courses[['course_id', 'course_name', 'rating', 'enrollment_numbers', 'feedback_score', 'popularity_score']].head(n)

#  Example Usage: Get Top 10
top_10 = get_top_popular_courses(10)
print(top_10)

"""**Content** **based** **filtering**"""

import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np


#  Select features
features = df_clean[['course_id','course_name','difficulty_level','course_duration_hours','course_price','certification_offered','study_material_available','feedback_score']]

#  Handle missing values
num_cols = ['course_duration_hours','course_price','feedback_score']
features[num_cols] = features[num_cols].fillna(features[num_cols].mean())
cat_cols = ['difficulty_level','certification_offered','study_material_available']
features[cat_cols] = features[cat_cols].fillna(features[cat_cols].mode().iloc[0])

#  Encode categorical features
encoder = OneHotEncoder()
encoded_cats = encoder.fit_transform(features[cat_cols]).toarray()
encoded_df = pd.DataFrame(encoded_cats, columns=encoder.get_feature_names_out(cat_cols))

#  Combine numeric + encoded categorical features
numeric_features = features[num_cols].reset_index(drop=True)
feature_matrix = pd.concat([numeric_features, encoded_df], axis=1).to_numpy()

# Scale features
scaler = StandardScaler()
feature_matrix_scaled = scaler.fit_transform(feature_matrix)

#  Memory-Safe Function to Compute Similarity Only for Target Course
def get_similar_courses(course_name, top_n=5):
    # Find the index of the course
    idx = df_clean[df_clean['course_name'].str.lower() == course_name.lower()].index[0]

    # Compute similarity of target course vector with all other courses (1 row vs all rows)
    target_vector = feature_matrix_scaled[idx].reshape(1, -1)
    similarities = cosine_similarity(target_vector, feature_matrix_scaled).flatten()

    # Sort by similarity
    top_indices = np.argsort(similarities)[::-1][1:top_n+1]

    return df_clean.iloc[top_indices][['course_id','course_name','difficulty_level','course_price','feedback_score']]

print(get_similar_courses("Python for Beginners", 5))

"""**Collaborative** **filering**"""

import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse import csr_matrix
import numpy as np


# Select user-item-rating data
ratings_df = df[['user_id','course_id','feedback_score']].dropna()

#  Create User-Course Matrix (rows=users, cols=courses)
user_course_matrix = ratings_df.pivot_table(index='user_id', columns='course_id', values='feedback_score').fillna(0)

# Convert to Sparse Matrix
sparse_matrix = csr_matrix(user_course_matrix.values)

# Compute Item-Item (Course-Course) Similarity
item_similarity = cosine_similarity(sparse_matrix.T)
item_similarity_df = pd.DataFrame(item_similarity, index=user_course_matrix.columns, columns=user_course_matrix.columns)

# Function to Recommend Similar Courses (Item-Based CF)
def recommend_cf_item_based(course_id, top_n=5):
    if course_id not in item_similarity_df.columns:
        return "Course not found in dataset"
    similar_scores = item_similarity_df[course_id].sort_values(ascending=False)[1:top_n+1]
    recommended_courses = df[df['course_id'].isin(similar_scores.index)][['course_id','course_name','difficulty_level','feedback_score']]
    return recommended_courses.drop_duplicates('course_id')

# Recommend courses similar to course_id 4061
print(recommend_cf_item_based(4061, 5))

"""using KNN"""

import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.neighbors import NearestNeighbors
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import numpy as np

# Select relevant features for similarity
features = ['difficulty_level', 'course_price', 'feedback_score', 'course_duration_hours']
course_ids = df['course_id'].values

X = df[features].copy()

#  Preprocess (One-Hot Encode categorical + Scale numeric)
categorical_cols = ['difficulty_level']
numeric_cols = ['course_price', 'feedback_score', 'course_duration_hours']

preprocessor = ColumnTransformer([
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),
    ('num', StandardScaler(), numeric_cols)
])

X_processed = preprocessor.fit_transform(X)

# Train KNN Model (Item-Based Similarity)
knn_model = NearestNeighbors(metric='cosine', algorithm='brute')
knn_model.fit(X_processed)

#  Recommendation Function
def recommend_knn(course_name, top_n=5):
    # Get index of course
    idx = df[df['course_name'].str.lower() == course_name.lower()].index[0]
    # Get feature vector for this course
    course_vector = X_processed[idx].reshape(1, -1)
    # Find nearest neighbors
    distances, indices = knn_model.kneighbors(course_vector, n_neighbors=top_n+1)
    # Retrieve recommended courses (excluding itself)
    recommended = df.iloc[indices.flatten()[1:top_n+1]][['course_id','course_name','difficulty_level','course_price','feedback_score']]
    recommended['similarity'] = 1 - distances.flatten()[1:top_n+1]  # cosine similarity = 1 - distance
    return recommended

# Example Usage
print(recommend_knn("Python for Beginners", 5))

"""Hybrid"""

import pickle
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.neighbors import NearestNeighbors


# Define features
features = ['difficulty_level', 'course_price', 'feedback_score', 'course_duration_hours']
cat_cols = ['difficulty_level']
num_cols = ['course_price', 'feedback_score', 'course_duration_hours']

# Preprocessing pipeline
preprocessor = ColumnTransformer([
    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),
    ('num', StandardScaler(), num_cols)
])

# Transform features
X_processed = preprocessor.fit_transform(df[features])

# Train KNN model
knn_model = NearestNeighbors(metric='cosine', algorithm='brute')
knn_model.fit(X_processed)

# Calculate normalized popularity score
df['popularity_score'] = (df['enrollment_numbers'] - df['enrollment_numbers'].min()) / \
(df['enrollment_numbers'].max() - df['enrollment_numbers'].min())

# Save everything needed
with open("preprocessor.pkl", "wb") as f:
    pickle.dump(preprocessor, f)

with open("knn_model.pkl", "wb") as f:
    pickle.dump(knn_model, f)

df.to_csv("courses_processed.csv", index=False)

print(" Model and data saved successfully!")

"""**Evaluation**

collaborative
"""

import numpy as np

def precision_at_k(predicted, actual, k=5):
    """ predicted: list of recommended item ids
        actual: set of items actually interacted by the user """
    predicted_k = predicted[:k]
    return len(set(predicted_k) & set(actual)) / k

def recall_at_k(predicted, actual, k=5):
    predicted_k = predicted[:k]
    return len(set(predicted_k) & set(actual)) / len(actual) if actual else 0

# Example usage
predicted = [101, 102, 103, 104, 105]  # recommended courses
actual = {101, 110, 103}               # actual user interactions
print("Precision@5:", precision_at_k(predicted, actual, 5))
print("Recall@5:", recall_at_k(predicted, actual, 5))

"""evaluation for KNN"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score


# Create binary target (e.g., enrolled if time_spent_hours > 1)
df['enrolled'] = (df['time_spent_hours'] > 1).astype(int)

# Features & Target
X = df[['course_price', 'feedback_score', 'previous_courses_taken']]
y = df['enrolled']

# Scale numeric features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Define and train KNN model
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)

# Predictions
y_pred = knn_model.predict(X_test)
y_prob = knn_model.predict_proba(X_test)[:, 1]

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))
print("ROC AUC:", roc_auc_score(y_test, y_prob))

"""evaluation for hybrid"""

import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# STEP 1: Prepare data
# Example: user-course interaction data
# df should have: user_id, course_id, enrolled (1/0), popularity_score, content_score
# popularity_score → from popularity model
# content_score → from content-based filtering

# Example dummy data (replace this with your real model outputs)
df = pd.DataFrame({
    'user_id': [1, 1, 2, 2, 3, 3],
    'course_id': [101, 102, 101, 103, 102, 104],
    'enrolled': [1, 0, 1, 0, 0, 1],
    'popularity_score': [0.9, 0.6, 0.8, 0.3, 0.4, 0.7],
    'content_score': [0.85, 0.65, 0.75, 0.25, 0.45, 0.80]
})

# STEP 2: Hybrid score calculation ----------
w1 = 0.3  # weight for popularity
w2 = 0.7  # weight for content-based

df['hybrid_score'] = (w1 * df['popularity_score']) + (w2 * df['content_score'])

# ---------- STEP 3: Convert to prediction (threshold 0.5) ----------
threshold = 0.5
df['hybrid_pred'] = (df['hybrid_score'] >= threshold).astype(int)

# ---------- STEP 4: Evaluate Hybrid Model ----------
accuracy = accuracy_score(df['enrolled'], df['hybrid_pred'])
precision = precision_score(df['enrolled'], df['hybrid_pred'])
recall = recall_score(df['enrolled'], df['hybrid_pred'])
f1 = f1_score(df['enrolled'], df['hybrid_pred'])

# ---------- STEP 5: Print results ----------
print("Hybrid Model Evaluation:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

"""**Deployment**"""

#pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import pickle
# import pandas as pd
# import streamlit as st
#
# # Load saved objects
# df = pd.read_csv("courses_processed.csv")
#
# with open("preprocessor.pkl", "rb") as f:
#     preprocessor = pickle.load(f)
#
# with open("knn_model.pkl", "rb") as f:
#     knn_model = pickle.load(f)
#
# # Recommendation function
# def hybrid_recommend(course_name, top_n=5, w1=0.7, w2=0.3):
#     try:
#         idx = df[df['course_name'].str.lower() == course_name.lower()].index[0]
#     except IndexError:
#         return pd.DataFrame({"Error": ["Course not found"]})
# 
#     course_vector = preprocessor.transform(df.loc[[idx], ['difficulty_level', 'course_price', 'feedback_score', 'course_duration_hours']])
# 
#     # Find neighbors
#     distances, indices = knn_model.kneighbors(course_vector, n_neighbors=top_n*5)
#     content_sim = 1 - distances.flatten()
#     neighbor_ids = indices.flatten()
# 
#     candidates = df.iloc[neighbor_ids].copy()
#     candidates['content_score'] = content_sim
#     candidates['hybrid_score'] = w1 * candidates['content_score'] + w2 * candidates['popularity_score']
# 
#     return candidates.sort_values(by='hybrid_score', ascending=False).head(top_n)[
#         ['course_id', 'course_name', 'difficulty_level', 'course_price', 'feedback_score', 'hybrid_score']
#     ]
# 
# # Streamlit UI
# st.title(" Hybrid Course Recommendation System")
# course_list = df['course_name'].tolist()
# selected_course = st.selectbox("Select a course:", course_list)
# if st.button("Recommend"):
#     recommendations = hybrid_recommend(selected_course, top_n=5)
#     st.dataframe(recommendations)

#!wget  -q -o - ipv4.canhanzip.com

#! streamlit run app.py & npx localtunnel --port 8501

